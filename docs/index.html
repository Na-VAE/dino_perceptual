<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DINO Perceptual Loss: A Better Alternative to LPIPS</title>
    <style>
        :root {
            --bg: #fafafa;
            --text: #333;
            --accent: #2563eb;
            --code-bg: #f3f4f6;
            --border: #e5e7eb;
            --green: #16a34a;
            --red: #dc2626;
        }

        * { box-sizing: border-box; margin: 0; padding: 0; }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.7;
            color: var(--text);
            background: var(--bg);
            max-width: 800px;
            margin: 0 auto;
            padding: 2rem 1.5rem;
        }

        h1 { font-size: 2.25rem; margin-bottom: 0.5rem; }
        h2 { font-size: 1.5rem; margin: 2.5rem 0 1rem; border-bottom: 2px solid var(--border); padding-bottom: 0.5rem; }
        h3 { font-size: 1.25rem; margin: 1.5rem 0 0.75rem; }

        p { margin-bottom: 1rem; }

        .subtitle {
            color: #666;
            font-size: 1.1rem;
            margin-bottom: 2rem;
        }

        a { color: var(--accent); }

        code {
            background: var(--code-bg);
            padding: 0.2em 0.4em;
            border-radius: 4px;
            font-size: 0.9em;
        }

        pre {
            background: #1e293b;
            color: #e2e8f0;
            padding: 1.25rem;
            border-radius: 8px;
            overflow-x: auto;
            margin: 1rem 0 1.5rem;
        }

        pre code {
            background: none;
            padding: 0;
            color: inherit;
        }

        .keyword { color: #c084fc; }
        .string { color: #4ade80; }
        .comment { color: #64748b; }
        .number { color: #fb923c; }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 1.5rem 0;
            font-size: 0.95rem;
        }

        th, td {
            padding: 0.75rem 1rem;
            text-align: left;
            border-bottom: 1px solid var(--border);
        }

        th { background: var(--code-bg); font-weight: 600; }

        .better { color: var(--green); font-weight: 600; }
        .worse { color: var(--red); }
        .best { background: #dcfce7; }

        figure {
            margin: 2rem 0;
        }

        figure img {
            width: 100%;
            border-radius: 8px;
            border: 1px solid var(--border);
        }

        figcaption {
            text-align: center;
            color: #666;
            font-size: 0.9rem;
            margin-top: 0.75rem;
        }

        .callout {
            background: #eff6ff;
            border-left: 4px solid var(--accent);
            padding: 1rem 1.25rem;
            margin: 1.5rem 0;
            border-radius: 0 8px 8px 0;
        }

        .eq {
            text-align: center;
            margin: 1.5rem 0;
            font-style: italic;
        }

        .install-box {
            background: var(--code-bg);
            border: 1px solid var(--border);
            border-radius: 8px;
            padding: 1.5rem;
            margin: 1.5rem 0;
        }

        footer {
            margin-top: 4rem;
            padding-top: 2rem;
            border-top: 1px solid var(--border);
            color: #666;
            font-size: 0.9rem;
        }
    </style>
</head>
<body>

<h1>DINO Perceptual Loss</h1>
<p class="subtitle">A better alternative to LPIPS using DINOv3 features for training image autoencoders and generative models</p>

<div class="install-box">
    <strong>Quick Install:</strong>
    <pre><code>pip install dino-perceptual</code></pre>
</div>

<h2>Background: What are Perceptual Losses?</h2>

<p>When training image reconstruction models (autoencoders, super-resolution, inpainting), the choice of loss function matters significantly. The simplest approach is <strong>pixel-wise losses</strong> like L1 or L2:</p>

<p class="eq">L = ||x - x&#770;||<sub>1</sub></p>

<p>While pixel losses are easy to optimize and yield high PSNR scores, they produce <strong>blurry outputs</strong>. This happens because pixel losses treat all errors equally&mdash;a small shift of an edge is penalized the same as completely wrong texture. The model learns to "hedge its bets" by outputting the average of plausible solutions, which is blurry.</p>

<h3>Perceptual Losses to the Rescue</h3>

<p><strong>Perceptual losses</strong> compare images in <em>feature space</em> rather than pixel space. The idea: extract features from a pretrained network (like VGG) and compare those instead:</p>

<p class="eq">L<sub>perceptual</sub> = ||&phi;(x) - &phi;(x&#770;)||<sub>2</sub></p>

<p>where &phi; extracts features from intermediate layers of a pretrained network. This captures high-level structure (edges, textures, semantics) rather than exact pixel values.</p>

<p>The most popular implementation is <strong>LPIPS</strong> (Learned Perceptual Image Patch Similarity), which uses VGG features with learned weights to match human perceptual judgments.</p>

<h2>Why DINO Instead of LPIPS?</h2>

<p>LPIPS uses VGG-16, a 2014 classification network. While it works, there are limitations:</p>

<ul style="margin: 1rem 0 1rem 1.5rem;">
    <li>VGG was trained for classification, not perceptual similarity</li>
    <li>Requires downloading separate pretrained weights</li>
    <li>Limited to fixed input sizes (224x224 native)</li>
    <li>Uses outdated CNN architecture (no attention mechanisms)</li>
</ul>

<p><strong>DINOv3</strong> is Meta's latest self-supervised vision foundation model (2025), trained on 1.7 billion images using modern Vision Transformer architectures. Key advantages:</p>

<ul style="margin: 1rem 0 1rem 1.5rem;">
    <li><strong>Modern architecture:</strong> Vision Transformers with attention, not 2014 CNNs</li>
    <li><strong>Massive scale:</strong> Trained on 1.7B images vs VGG's 1.2M ImageNet images</li>
    <li><strong>Self-supervised:</strong> Learns visual structure, not classification labels</li>
    <li><strong>Rich features:</strong> Produces semantically meaningful representations naturally</li>
</ul>

<div class="callout">
    <strong>Key finding:</strong> Replacing LPIPS with DINO loss achieves <strong>2x better perceptual metrics</strong> (rFID, FDD) with no additional complexity.
</div>

<h2>Empirical Results</h2>

<p>We ablated different loss configurations when training a vision transformer autoencoder (ViTok). All experiments use the same Td4-T architecture with Charbonnier pixel loss as the baseline:</p>

<table>
    <thead>
        <tr>
            <th>Loss Configuration</th>
            <th>rFID &darr;</th>
            <th>rFDD &darr;</th>
            <th>PSNR &uarr;</th>
            <th>SSIM &uarr;</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td><em>Baseline (pixel only)</em></td>
            <td>2.13</td>
            <td>6.96</td>
            <td class="best">34.31</td>
            <td class="best">0.927</td>
        </tr>
        <tr>
            <td>+ LPIPS (&lambda;=0.1)</td>
            <td>0.72 <span class="better">(-1.41)</span></td>
            <td>2.93 <span class="better">(-4.03)</span></td>
            <td>34.19</td>
            <td>0.923</td>
        </tr>
        <tr>
            <td>+ DINO (&lambda;=250)</td>
            <td>0.51 <span class="better">(-1.62)</span></td>
            <td>1.45 <span class="better">(-5.51)</span></td>
            <td>33.93</td>
            <td>0.919</td>
        </tr>
        <tr style="background: #f0fdf4;">
            <td><strong>+ DINO (&lambda;=1000)</strong></td>
            <td class="best"><strong>0.30</strong> <span class="better">(-1.83)</span></td>
            <td class="best"><strong>1.12</strong> <span class="better">(-5.84)</span></td>
            <td>33.64 <span class="worse">(-0.67)</span></td>
            <td>0.914</td>
        </tr>
    </tbody>
</table>

<p><strong>Key observations:</strong></p>
<ul style="margin: 1rem 0 1rem 1.5rem;">
    <li>DINO achieves <strong>7x better rFID</strong> (0.30 vs 2.13) compared to pixel-only</li>
    <li>DINO achieves <strong>2x better rFID</strong> than LPIPS (0.30 vs 0.72)</li>
    <li>The cost is only 0.67 dB PSNR&mdash;invisible to human perception</li>
    <li>Higher DINO weight trades more PSNR for better perceptual metrics</li>
</ul>

<h2>Gradient Visualization</h2>

<p>To understand <em>why</em> DINO works better, we can visualize the gradient of each loss with respect to a distorted image. This shows "where the loss focuses":</p>

<figure>
    <img src="fig_gradients.png" alt="Gradient visualization comparing L1, DINO, and LPIPS">
    <figcaption>Gradient magnitude for each loss type. L1 is uniform; DINO and LPIPS focus on semantically meaningful regions.</figcaption>
</figure>

<ul style="margin: 1rem 0 1rem 1.5rem;">
    <li><strong>L1 gradient:</strong> Nearly uniform&mdash;every pixel matters equally</li>
    <li><strong>DINO gradient:</strong> Focuses on semantic structure (edges, textures, object parts)</li>
    <li><strong>LPIPS gradient:</strong> Similar pattern but less pronounced than DINO</li>
</ul>

<h3>Gradient Overlay</h3>

<p>We can also overlay these gradients on the image to see exactly which regions each loss prioritizes:</p>

<figure>
    <img src="fig_gradient_overlay.png" alt="Gradient overlay visualization">
    <figcaption>Gradient magnitude overlaid on the distorted image. Bright regions indicate where the loss applies the strongest correction signal.</figcaption>
</figure>

<h3>Across Different Distortions</h3>

<p>Different distortion types (blur, noise, JPEG compression, color shift) reveal different gradient patterns:</p>

<figure>
    <img src="fig_distortions.png" alt="Gradient comparison across distortion types">
    <figcaption>Gradient saliency across four distortion types. DINO consistently focuses on semantically meaningful regions regardless of distortion type.</figcaption>
</figure>

<h2>Usage</h2>

<p>Using DINO perceptual loss is straightforward:</p>

<pre><code><span class="keyword">import</span> torch
<span class="keyword">from</span> dino_perceptual <span class="keyword">import</span> DINOPerceptual
<span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn

<span class="comment"># Initialize with bfloat16 and torch.compile for best performance</span>
perceptual_loss = DINOPerceptual(model_size=<span class="string">"B"</span>).cuda().bfloat16().eval()
perceptual_loss = torch.compile(perceptual_loss, fullgraph=<span class="keyword">True</span>)

<span class="comment"># In your training loop</span>
reconstructed = autoencoder(images)

l1_loss = nn.functional.l1_loss(reconstructed, images)
dino_loss = perceptual_loss(reconstructed, images).mean()

<span class="comment"># Combined loss (weight 250-1000 for DINO)</span>
total_loss = l1_loss + <span class="number">250.0</span> * dino_loss</code></pre>

<p>The DINO model is automatically downloaded from HuggingFace on first use. Images should be tensors in <code>[-1, 1]</code> range. Use <code>version="v2"</code> for legacy DINOv2 models.</p>

<h2>Installation</h2>

<div class="install-box">
<pre><code><span class="comment"># From PyPI</span>
pip install dino-perceptual

<span class="comment"># From source</span>
git clone https://github.com/Na-VAE/dino-perceptual.git
cd dino-perceptual
pip install -e .</code></pre>
</div>

<h2>Citation</h2>

<pre><code>@software{dino_perceptual,
  title={DINO Perceptual Loss},
  author={Hansen-Estruch, Philippe and Chen, Jiahui and Ramanujan, Vivek
          and Zohar, Orr and Ping, Yan and Sinha, Animesh and Georgopoulos,
          Markos and Schoenfeld, Edgar and Hou, Ji and Juefei-Xu, Felix
          and Vishwanath, Sriram and Thabet, Ali},
  year={2025},
  url={https://github.com/Na-VAE/dino-perceptual}
}</code></pre>

<footer>
    <p>Part of the <a href="https://github.com/Na-VAE">Na-VAE</a> project. Built on <a href="https://github.com/facebookresearch/dinov2">DINOv2</a> by Meta AI.</p>
</footer>

</body>
</html>
