<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DINO Perceptual Loss: A Better Alternative to LPIPS</title>
    <style>
        :root {
            --bg: #fafafa;
            --text: #333;
            --accent: #2563eb;
            --code-bg: #f3f4f6;
            --border: #e5e7eb;
            --green: #16a34a;
            --red: #dc2626;
        }

        * { box-sizing: border-box; margin: 0; padding: 0; }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.7;
            color: var(--text);
            background: var(--bg);
            max-width: 800px;
            margin: 0 auto;
            padding: 2rem 1.5rem;
        }

        h1 { font-size: 2.25rem; margin-bottom: 0.5rem; }
        h2 { font-size: 1.5rem; margin: 2.5rem 0 1rem; border-bottom: 2px solid var(--border); padding-bottom: 0.5rem; }
        h3 { font-size: 1.25rem; margin: 1.5rem 0 0.75rem; }

        p { margin-bottom: 1rem; }

        .subtitle {
            color: #666;
            font-size: 1.1rem;
            margin-bottom: 0.5rem;
        }

        .author {
            color: #888;
            font-size: 0.95rem;
            margin-bottom: 2rem;
        }

        a { color: var(--accent); }

        code {
            background: var(--code-bg);
            padding: 0.2em 0.4em;
            border-radius: 4px;
            font-size: 0.9em;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 1.5rem 0;
            font-size: 0.95rem;
        }

        th, td {
            padding: 0.75rem 1rem;
            text-align: left;
            border-bottom: 1px solid var(--border);
        }

        th { background: var(--code-bg); font-weight: 600; }

        .better { color: var(--green); font-weight: 600; }
        .worse { color: var(--red); }
        .best { background: #dcfce7; }

        .callout {
            background: #eff6ff;
            border-left: 4px solid var(--accent);
            padding: 1rem 1.25rem;
            margin: 1.5rem 0;
            border-radius: 0 8px 8px 0;
        }

        .eq {
            text-align: center;
            margin: 1.5rem 0;
            font-style: italic;
        }

        footer {
            margin-top: 4rem;
            padding-top: 2rem;
            border-top: 1px solid var(--border);
            color: #666;
            font-size: 0.9rem;
        }
    </style>
</head>
<body>

<h1>DINO Perceptual Loss</h1>
<p class="subtitle">A better alternative to LPIPS for training image autoencoders and generative models</p>
<p class="author">Philippe Hansen-Estruch</p>

<h2>Background: What are Perceptual Losses?</h2>

<p>When training image reconstruction models (autoencoders, super-resolution, inpainting), the choice of loss function matters significantly. The simplest approach is <strong>pixel-wise losses</strong> like L1 or L2:</p>

<p class="eq">L = ||x - x&#770;||<sub>1</sub></p>

<p>While pixel losses are easy to optimize and yield high PSNR scores, they produce <strong>blurry outputs</strong>. This happens because pixel losses treat all errors equally&mdash;a small shift of an edge is penalized the same as completely wrong texture. The model learns to "hedge its bets" by outputting the average of plausible solutions, which is blurry.</p>

<h3>Perceptual Losses to the Rescue</h3>

<p><strong>Perceptual losses</strong> compare images in <em>feature space</em> rather than pixel space. The idea: extract features from a pretrained network (like VGG) and compare those instead:</p>

<p class="eq">L<sub>perceptual</sub> = ||&phi;(x) - &phi;(x&#770;)||<sub>2</sub></p>

<p>where &phi; extracts features from intermediate layers of a pretrained network. This captures high-level structure (edges, textures, semantics) rather than exact pixel values.</p>

<p>The most popular implementation is <strong>LPIPS</strong> (Learned Perceptual Image Patch Similarity) <a href="#ref-lpips">[5]</a>, which uses VGG features with learned weights to match human perceptual judgments.</p>

<h2>Why DINO Instead of LPIPS?</h2>

<p>LPIPS uses VGG-16, a 2014 classification network. While it works, there are limitations:</p>

<ul style="margin: 1rem 0 1rem 1.5rem;">
    <li>VGG was trained for classification, not perceptual similarity</li>
    <li>Limited to fixed input sizes (224x224 native)</li>
    <li>Uses outdated CNN architecture (no attention mechanisms)</li>
    <li>Trained on only 1.2M ImageNet images</li>
</ul>

<p><strong>DINOv2</strong> <a href="#ref-dino">[6]</a> and <strong>DINOv3</strong> <a href="#ref-dinov3">[7]</a> are Meta's self-supervised vision foundation models, trained on up to 1.7 billion images using modern Vision Transformer architectures. Key advantages:</p>

<ul style="margin: 1rem 0 1rem 1.5rem;">
    <li><strong>Modern architecture:</strong> Vision Transformers with attention, not 2014 CNNs</li>
    <li><strong>Massive scale:</strong> Trained on 1.7B images vs VGG's 1.2M ImageNet images</li>
    <li><strong>Self-supervised:</strong> Learns visual structure through self-distillation, not classification labels</li>
    <li><strong>Rich features:</strong> Produces semantically meaningful representations naturally</li>
</ul>

<div class="callout">
    <strong>Key finding:</strong> Replacing LPIPS with DINO loss achieves <strong>2&times; better perceptual metrics</strong> (rFID, rFDD) with no additional complexity.
</div>

<h2>Related Work</h2>

<h3>Perceptual Loss Functions</h3>

<p>Johnson et al. <a href="#ref-johnson">[8]</a> introduced perceptual losses using VGG features for style transfer and super-resolution. Zhang et al. <a href="#ref-lpips">[5]</a> extended this with LPIPS, learning weights on VGG features to better match human perception. These approaches established perceptual losses as essential for generative models.</p>

<h3>Self-Supervised Visual Representations</h3>

<p>DINO <a href="#ref-dino-v1">[9]</a> demonstrated that self-supervised Vision Transformers learn semantically meaningful features without labels. DINOv2 <a href="#ref-dino">[6]</a> scaled this to 1.7B images with improved training recipes. DINOv3 <a href="#ref-dinov3">[7]</a> further refined the approach with modern architectural improvements. These models produce features that naturally capture perceptual similarity.</p>

<h3>Visual Tokenizers for Generative Models</h3>

<p>Latent diffusion models <a href="#ref-ldm">[1]</a> popularized the use of autoencoders to compress images into latent spaces for efficient generation. Recent work has explored scaling these tokenizers: FLUX <a href="#ref-flux">[4]</a> uses high-capacity VAEs, while ViTok <a href="#ref-vitok">[10]</a> demonstrates benefits of Vision Transformer architectures for tokenization. Our work shows that the choice of perceptual loss is equally important as architectural improvements.</p>

<h2>Empirical Results</h2>

<h3>Comparison with State-of-the-Art VAEs</h3>

<p>Reconstruction quality on <strong>ImageNet 256&times;256</strong> (center crop). ViTok-v2 models trained with <strong>DINO perceptual loss (no GAN)</strong> achieve state-of-the-art results:</p>

<table>
    <thead>
        <tr>
            <th>Model</th>
            <th>Compression</th>
            <th>rFID &darr;</th>
            <th>rFDD &darr;</th>
            <th>PSNR &uarr;</th>
            <th>SSIM &uarr;</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>RAE <a href="#ref-rae">[1]</a></td>
            <td>none</td>
            <td>0.61</td>
            <td>&mdash;</td>
            <td>18.83</td>
            <td>0.496</td>
        </tr>
        <tr>
            <td>SD-VAE <a href="#ref-sd">[2]</a></td>
            <td>f8&times;4ch</td>
            <td>0.73</td>
            <td>6.14</td>
            <td>25.70</td>
            <td>0.702</td>
        </tr>
        <tr>
            <td>Qwen VAE <a href="#ref-qwen">[3]</a></td>
            <td>f8&times;16ch</td>
            <td>1.32</td>
            <td>7.36</td>
            <td>30.27</td>
            <td>0.860</td>
        </tr>
        <tr>
            <td>FLUX.1* <a href="#ref-flux">[4]</a></td>
            <td>f8&times;16ch</td>
            <td class="best"><strong>0.15</strong></td>
            <td>2.29</td>
            <td>31.10</td>
            <td>0.887</td>
        </tr>
        <tr>
            <td>FLUX.2 <a href="#ref-flux">[4]</a></td>
            <td>f8&times;16ch</td>
            <td>0.27</td>
            <td>&mdash;</td>
            <td>31.46</td>
            <td>0.904</td>
        </tr>
        <tr style="background: #f0fdf4;">
            <td><strong>ViTok-v2 16&times;16</strong></td>
            <td>f16&times;16ch</td>
            <td>1.32</td>
            <td>3.13</td>
            <td>28.46</td>
            <td>0.793</td>
        </tr>
        <tr style="background: #f0fdf4;">
            <td><strong>ViTok-v2 16&times;32</strong></td>
            <td>f16&times;32ch</td>
            <td>1.06</td>
            <td>2.36</td>
            <td>31.23</td>
            <td>0.867</td>
        </tr>
        <tr style="background: #dcfce7;">
            <td><strong>ViTok-v2 16&times;64</strong></td>
            <td>f16&times;64ch</td>
            <td>0.64</td>
            <td class="best"><strong>1.70</strong></td>
            <td class="best"><strong>34.05</strong></td>
            <td class="best"><strong>0.921</strong></td>
        </tr>
    </tbody>
</table>

<div class="callout">
    <strong>No GAN required:</strong> ViTok-v2 achieves these results using only L1 (Charbonnier) + DINO perceptual loss + scaling losses (SSIM, FFT). No adversarial training needed, which simplifies training and improves stability.
</div>

<p style="font-size: 0.9rem; color: #666; margin-top: 0.5rem;">*Self-reproduced results. ViTok uses 4&times; fewer tokens (256 vs 1024) than f8-based methods, enabling faster diffusion training. All ViTok-v2 models are 4.5B parameters.</p>

<h3>Loss Ablation</h3>

<p>Effect of different loss configurations on our <strong>4.5B parameter ViTok-v2 autoencoder</strong>:</p>

<table>
    <thead>
        <tr>
            <th>Loss Configuration</th>
            <th>rFID &darr;</th>
            <th>rFDD &darr;</th>
            <th>PSNR &uarr;</th>
            <th>SSIM &uarr;</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Pixel-only (Charbonnier)</td>
            <td>2.13</td>
            <td>6.96</td>
            <td><strong>34.31</strong></td>
            <td>0.925</td>
        </tr>
        <tr>
            <td>+ SSIM</td>
            <td>1.99</td>
            <td>6.76</td>
            <td>34.29</td>
            <td><strong>0.927</strong></td>
        </tr>
        <tr>
            <td>+ LPIPS</td>
            <td>0.72</td>
            <td>2.93</td>
            <td>34.19</td>
            <td>0.923</td>
        </tr>
        <tr>
            <td>+ DINO</td>
            <td><strong>0.30</strong></td>
            <td><strong>1.12</strong></td>
            <td>33.64</td>
            <td>0.914</td>
        </tr>
    </tbody>
</table>

<p>DINO achieves <strong>7&times; better rFID</strong> and <strong>6&times; better rFDD</strong> than pixel-only, and <strong>2&times; better</strong> than LPIPS, with only ~0.7 dB PSNR trade-off.</p>

<h2>Conclusion</h2>

<p>DINO perceptual loss offers a simple drop-in replacement for LPIPS that leverages modern self-supervised visual representations. By using features from models trained on billions of images with Vision Transformer architectures, we achieve substantially better perceptual quality metrics without requiring adversarial training or complex loss balancing.</p>

<p>The code is available at <a href="https://github.com/Na-VAE/dino-perceptual">github.com/Na-VAE/dino-perceptual</a>.</p>

<h2>References</h2>

<ol style="font-size: 0.9rem; margin: 1rem 0 1rem 1.5rem;">
    <li id="ref-rae">Rombach et al. "High-Resolution Image Synthesis with Latent Diffusion Models." CVPR 2022. <a href="https://arxiv.org/abs/2112.10752">arXiv:2112.10752</a></li>
    <li id="ref-sd">Rombach et al. "High-Resolution Image Synthesis with Latent Diffusion Models." CVPR 2022. (SD-VAE) <a href="https://github.com/CompVis/stable-diffusion">GitHub</a></li>
    <li id="ref-qwen">Qwen Team. "Qwen-VL: A Versatile Vision-Language Model." 2023. <a href="https://arxiv.org/abs/2308.12966">arXiv:2308.12966</a></li>
    <li id="ref-flux">Black Forest Labs. "FLUX.1" 2024. <a href="https://blackforestlabs.ai/">blackforestlabs.ai</a></li>
    <li id="ref-lpips">Zhang et al. "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric." CVPR 2018. <a href="https://arxiv.org/abs/1801.03924">arXiv:1801.03924</a></li>
    <li id="ref-dino">Oquab et al. "DINOv2: Learning Robust Visual Features without Supervision." TMLR 2024. <a href="https://arxiv.org/abs/2304.07193">arXiv:2304.07193</a></li>
    <li id="ref-dinov3">Simeoni et al. "DINOv3: Towards Scalable Self-Supervised Visual Representation Learning." 2025.</li>
    <li id="ref-johnson">Johnson et al. "Perceptual Losses for Real-Time Style Transfer and Super-Resolution." ECCV 2016. <a href="https://arxiv.org/abs/1603.08155">arXiv:1603.08155</a></li>
    <li id="ref-dino-v1">Caron et al. "Emerging Properties in Self-Supervised Vision Transformers." ICCV 2021. <a href="https://arxiv.org/abs/2104.14294">arXiv:2104.14294</a></li>
    <li id="ref-vitok">Hansen-Estruch et al. "ViTok: Learning to Scale Visual Tokenizers." 2025.</li>
</ol>

<footer>
    <p>Part of the <a href="https://github.com/Na-VAE">Na-VAE</a> project. Built on <a href="https://github.com/facebookresearch/dinov2">DINOv2</a> and <a href="https://github.com/facebookresearch/dinov3">DINOv3</a> by Meta AI.</p>
</footer>

</body>
</html>
