<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DINO Perceptual Loss: A Better Alternative to LPIPS</title>
    <style>
        :root {
            --bg: #fafafa;
            --text: #333;
            --accent: #2563eb;
            --code-bg: #f3f4f6;
            --border: #e5e7eb;
            --green: #16a34a;
            --red: #dc2626;
        }

        * { box-sizing: border-box; margin: 0; padding: 0; }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.7;
            color: var(--text);
            background: var(--bg);
            max-width: 800px;
            margin: 0 auto;
            padding: 2rem 1.5rem;
        }

        h1 { font-size: 2.25rem; margin-bottom: 0.5rem; }
        h2 { font-size: 1.5rem; margin: 2.5rem 0 1rem; border-bottom: 2px solid var(--border); padding-bottom: 0.5rem; }
        h3 { font-size: 1.25rem; margin: 1.5rem 0 0.75rem; }

        p { margin-bottom: 1rem; }

        .subtitle {
            color: #666;
            font-size: 1.1rem;
            margin-bottom: 2rem;
        }

        a { color: var(--accent); }

        code {
            background: var(--code-bg);
            padding: 0.2em 0.4em;
            border-radius: 4px;
            font-size: 0.9em;
        }

        pre {
            background: #1e293b;
            color: #e2e8f0;
            padding: 1.25rem;
            border-radius: 8px;
            overflow-x: auto;
            margin: 1rem 0 1.5rem;
        }

        pre code {
            background: none;
            padding: 0;
            color: inherit;
        }

        .keyword { color: #c084fc; }
        .string { color: #4ade80; }
        .comment { color: #64748b; }
        .number { color: #fb923c; }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 1.5rem 0;
            font-size: 0.95rem;
        }

        th, td {
            padding: 0.75rem 1rem;
            text-align: left;
            border-bottom: 1px solid var(--border);
        }

        th { background: var(--code-bg); font-weight: 600; }

        .better { color: var(--green); font-weight: 600; }
        .worse { color: var(--red); }
        .best { background: #dcfce7; }

        figure {
            margin: 2rem 0;
        }

        figure img {
            width: 100%;
            border-radius: 8px;
            border: 1px solid var(--border);
        }

        figcaption {
            text-align: center;
            color: #666;
            font-size: 0.9rem;
            margin-top: 0.75rem;
        }

        .callout {
            background: #eff6ff;
            border-left: 4px solid var(--accent);
            padding: 1rem 1.25rem;
            margin: 1.5rem 0;
            border-radius: 0 8px 8px 0;
        }

        .eq {
            text-align: center;
            margin: 1.5rem 0;
            font-style: italic;
        }

        .install-box {
            background: var(--code-bg);
            border: 1px solid var(--border);
            border-radius: 8px;
            padding: 1.5rem;
            margin: 1.5rem 0;
        }

        footer {
            margin-top: 4rem;
            padding-top: 2rem;
            border-top: 1px solid var(--border);
            color: #666;
            font-size: 0.9rem;
        }
    </style>
</head>
<body>

<h1>DINO Perceptual Loss</h1>
<p class="subtitle">A better alternative to LPIPS using DINOv3 features for training image autoencoders and generative models</p>

<div class="install-box">
    <strong>Quick Install:</strong>
    <pre><code>pip install dino-perceptual</code></pre>
</div>

<h2>Background: What are Perceptual Losses?</h2>

<p>When training image reconstruction models (autoencoders, super-resolution, inpainting), the choice of loss function matters significantly. The simplest approach is <strong>pixel-wise losses</strong> like L1 or L2:</p>

<p class="eq">L = ||x - x&#770;||<sub>1</sub></p>

<p>While pixel losses are easy to optimize and yield high PSNR scores, they produce <strong>blurry outputs</strong>. This happens because pixel losses treat all errors equally&mdash;a small shift of an edge is penalized the same as completely wrong texture. The model learns to "hedge its bets" by outputting the average of plausible solutions, which is blurry.</p>

<h3>Perceptual Losses to the Rescue</h3>

<p><strong>Perceptual losses</strong> compare images in <em>feature space</em> rather than pixel space. The idea: extract features from a pretrained network (like VGG) and compare those instead:</p>

<p class="eq">L<sub>perceptual</sub> = ||&phi;(x) - &phi;(x&#770;)||<sub>2</sub></p>

<p>where &phi; extracts features from intermediate layers of a pretrained network. This captures high-level structure (edges, textures, semantics) rather than exact pixel values.</p>

<p>The most popular implementation is <strong>LPIPS</strong> (Learned Perceptual Image Patch Similarity), which uses VGG features with learned weights to match human perceptual judgments.</p>

<h2>Why DINO Instead of LPIPS?</h2>

<p>LPIPS uses VGG-16, a 2014 classification network. While it works, there are limitations:</p>

<ul style="margin: 1rem 0 1rem 1.5rem;">
    <li>VGG was trained for classification, not perceptual similarity</li>
    <li>Requires downloading separate pretrained weights</li>
    <li>Limited to fixed input sizes (224x224 native)</li>
    <li>Uses outdated CNN architecture (no attention mechanisms)</li>
</ul>

<p><strong>DINOv3</strong> is Meta's latest self-supervised vision foundation model (2025), trained on 1.7 billion images using modern Vision Transformer architectures. Key advantages:</p>

<ul style="margin: 1rem 0 1rem 1.5rem;">
    <li><strong>Modern architecture:</strong> Vision Transformers with attention, not 2014 CNNs</li>
    <li><strong>Massive scale:</strong> Trained on 1.7B images vs VGG's 1.2M ImageNet images</li>
    <li><strong>Self-supervised:</strong> Learns visual structure, not classification labels</li>
    <li><strong>Rich features:</strong> Produces semantically meaningful representations naturally</li>
</ul>

<div class="callout">
    <strong>Key finding:</strong> Replacing LPIPS with DINO loss achieves <strong>2x better perceptual metrics</strong> (rFID, FDD) with no additional complexity.
</div>

<h2>Empirical Results</h2>

<h3>Comparison with State-of-the-Art VAEs</h3>

<p>Reconstruction quality on <strong>ImageNet 256&times;256</strong> (center crop). ViTok-v2 models trained with <strong>DINO perceptual loss (no GAN)</strong> achieve state-of-the-art results using only L1 + DINO + scaling losses:</p>

<table>
    <thead>
        <tr>
            <th>Model</th>
            <th>Compression</th>
            <th>rFID &darr;</th>
            <th>rFDD &darr;</th>
            <th>PSNR &uarr;</th>
            <th>SSIM &uarr;</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>RAE <a href="#ref-rae">[1]</a></td>
            <td>none</td>
            <td>0.61</td>
            <td>&mdash;</td>
            <td>18.83</td>
            <td>0.496</td>
        </tr>
        <tr>
            <td>SD-VAE <a href="#ref-sd">[2]</a></td>
            <td>f8&times;4ch</td>
            <td>0.73</td>
            <td>6.14</td>
            <td>25.70</td>
            <td>0.702</td>
        </tr>
        <tr>
            <td>Qwen VAE <a href="#ref-qwen">[3]</a></td>
            <td>f8&times;16ch</td>
            <td>1.32</td>
            <td>7.36</td>
            <td>30.27</td>
            <td>0.860</td>
        </tr>
        <tr>
            <td>FLUX.1* <a href="#ref-flux">[4]</a></td>
            <td>f8&times;16ch</td>
            <td class="best"><strong>0.15</strong></td>
            <td>2.29</td>
            <td>31.10</td>
            <td>0.887</td>
        </tr>
        <tr>
            <td>FLUX.2 <a href="#ref-flux">[4]</a></td>
            <td>f8&times;16ch</td>
            <td>0.27</td>
            <td>&mdash;</td>
            <td>31.46</td>
            <td>0.904</td>
        </tr>
        <tr style="background: #f0fdf4;">
            <td><strong>ViTok-v2 Td4-T/16&times;16</strong></td>
            <td>f16&times;16ch</td>
            <td>1.32</td>
            <td>3.13</td>
            <td>28.46</td>
            <td>0.793</td>
        </tr>
        <tr style="background: #f0fdf4;">
            <td><strong>ViTok-v2 Td4-T/16&times;32</strong></td>
            <td>f16&times;32ch</td>
            <td>1.06</td>
            <td>2.36</td>
            <td>31.23</td>
            <td>0.867</td>
        </tr>
        <tr style="background: #dcfce7;">
            <td><strong>ViTok-v2 Td4-T/16&times;64</strong></td>
            <td>f16&times;64ch</td>
            <td>0.64</td>
            <td class="best"><strong>1.70</strong></td>
            <td class="best"><strong>34.05</strong></td>
            <td class="best"><strong>0.921</strong></td>
        </tr>
    </tbody>
</table>

<div class="callout">
    <strong>No GAN required:</strong> ViTok-v2 achieves these results using only L1 (Charbonnier) + DINO perceptual loss + scaling losses (SSIM, FFT). No adversarial training needed, which simplifies training and improves stability.
</div>

<p style="font-size: 0.9rem; color: #666; margin-top: 0.5rem;">*Self-reproduced results. ViTok uses 4&times; fewer tokens (256 vs 1024) than f8-based methods, enabling faster diffusion training. All ViTok-v2 models are 4.5B parameters (Td4-T configuration).</p>

<h3>Loss Ablation</h3>

<p>Effect of different loss configurations on our <strong>4.5B parameter ViTok-v2 autoencoder</strong> (Td4-T/16&times;64):</p>

<table>
    <thead>
        <tr>
            <th>Loss Configuration</th>
            <th>rFID &darr;</th>
            <th>rFDD &darr;</th>
            <th>PSNR &uarr;</th>
            <th>SSIM &uarr;</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Pixel-only (Charbonnier)</td>
            <td>2.13</td>
            <td>6.96</td>
            <td><strong>34.31</strong></td>
            <td>0.925</td>
        </tr>
        <tr>
            <td>+ SSIM</td>
            <td>1.99</td>
            <td>6.76</td>
            <td>34.29</td>
            <td><strong>0.927</strong></td>
        </tr>
        <tr>
            <td>+ LPIPS</td>
            <td>0.72</td>
            <td>2.93</td>
            <td>34.19</td>
            <td>0.923</td>
        </tr>
        <tr>
            <td>+ DINO</td>
            <td><strong>0.30</strong></td>
            <td><strong>1.12</strong></td>
            <td>33.64</td>
            <td>0.914</td>
        </tr>
    </tbody>
</table>

<p>DINO achieves <strong>7&times; better rFID</strong> and <strong>6&times; better rFDD</strong> than pixel-only, and <strong>2&times; better</strong> than LPIPS, with only ~0.7 dB PSNR trade-off.</p>

<h2>Usage</h2>

<p>Using DINO perceptual loss is straightforward:</p>

<pre><code><span class="keyword">import</span> torch
<span class="keyword">from</span> dino_perceptual <span class="keyword">import</span> DINOPerceptual
<span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn

<span class="comment"># Initialize with bfloat16 and torch.compile for best performance</span>
perceptual_loss = DINOPerceptual(model_size=<span class="string">"B"</span>).cuda().bfloat16().eval()
perceptual_loss = torch.compile(perceptual_loss, fullgraph=<span class="keyword">True</span>)

<span class="comment"># In your training loop</span>
reconstructed = autoencoder(images)

l1_loss = nn.functional.l1_loss(reconstructed, images)
dino_loss = perceptual_loss(reconstructed, images).mean()

<span class="comment"># Combined loss (weight 250-1000 for DINO)</span>
total_loss = l1_loss + <span class="number">250.0</span> * dino_loss</code></pre>

<p>The DINO model is automatically downloaded from HuggingFace on first use. Images should be tensors in <code>[-1, 1]</code> range. Use <code>version="v2"</code> for legacy DINOv2 models.</p>

<h2>Installation</h2>

<div class="install-box">
<pre><code><span class="comment"># From PyPI</span>
pip install dino-perceptual

<span class="comment"># From source</span>
git clone https://github.com/Na-VAE/dino-perceptual.git
cd dino-perceptual
pip install -e .</code></pre>
</div>

<h3>HuggingFace Access Setup</h3>

<p>DINOv3 and DINOv2 models are hosted on HuggingFace and require accepting the model license:</p>

<ol style="margin: 1rem 0 1rem 1.5rem;">
    <li><strong>Accept the license:</strong> Visit the model page (<a href="https://huggingface.co/facebook/dinov3-vitb16-pretrain-lvd1689m">DINOv3</a> or <a href="https://huggingface.co/facebook/dinov2-base">DINOv2</a>) and click "Agree and access repository"</li>
    <li><strong>Create a token:</strong> Go to <a href="https://huggingface.co/settings/tokens">HuggingFace Settings</a> and create a "Read" access token</li>
    <li><strong>Authenticate:</strong> Use one of the methods below</li>
</ol>

<pre><code><span class="comment"># Option A: Environment variable</span>
export HF_TOKEN=<span class="string">"hf_your_token_here"</span>

<span class="comment"># Option B: HuggingFace CLI</span>
huggingface-cli login

<span class="comment"># Option C: In Python</span>
<span class="keyword">from</span> huggingface_hub <span class="keyword">import</span> login
login(token=<span class="string">"hf_your_token_here"</span>)</code></pre>

<p>For <strong>Modal</strong> deployments, create a secret and reference it:</p>

<pre><code><span class="comment"># Create secret: modal secret create huggingface HF_TOKEN=hf_...</span>
@app.function(secrets=[modal.Secret.from_name(<span class="string">"huggingface"</span>)], gpu=<span class="string">"A10G"</span>)
<span class="keyword">def</span> run():
    loss_fn = DINOPerceptual().cuda()  <span class="comment"># HF_TOKEN auto-loaded</span></code></pre>

<h2>Citation</h2>

<pre><code>@software{dino_perceptual,
  title={DINO Perceptual Loss},
  author={Hansen-Estruch, Philippe and Chen, Jiahui and Ramanujan, Vivek
          and Zohar, Orr and Ping, Yan and Sinha, Animesh and Georgopoulos,
          Markos and Schoenfeld, Edgar and Hou, Ji and Juefei-Xu, Felix
          and Vishwanath, Sriram and Thabet, Ali},
  year={2025},
  url={https://github.com/Na-VAE/dino-perceptual}
}</code></pre>

<h2>References</h2>

<ol style="font-size: 0.9rem; margin: 1rem 0 1rem 1.5rem;">
    <li id="ref-rae">Rombach et al. "High-Resolution Image Synthesis with Latent Diffusion Models." CVPR 2022. <a href="https://arxiv.org/abs/2112.10752">arXiv:2112.10752</a></li>
    <li id="ref-sd">Rombach et al. "High-Resolution Image Synthesis with Latent Diffusion Models." CVPR 2022. (SD-VAE) <a href="https://github.com/CompVis/stable-diffusion">GitHub</a></li>
    <li id="ref-qwen">Qwen Team. "Qwen-VL: A Versatile Vision-Language Model." 2023. <a href="https://arxiv.org/abs/2308.12966">arXiv:2308.12966</a></li>
    <li id="ref-flux">Black Forest Labs. "FLUX.1" 2024. <a href="https://blackforestlabs.ai/">blackforestlabs.ai</a></li>
    <li id="ref-lpips">Zhang et al. "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric." CVPR 2018. <a href="https://arxiv.org/abs/1801.03924">arXiv:1801.03924</a></li>
    <li id="ref-dino">Oquab et al. "DINOv2: Learning Robust Visual Features without Supervision." TMLR 2024. <a href="https://arxiv.org/abs/2304.07193">arXiv:2304.07193</a></li>
</ol>

<footer>
    <p>Part of the <a href="https://github.com/Na-VAE">Na-VAE</a> project. Built on <a href="https://github.com/facebookresearch/dinov2">DINOv2</a> and <a href="https://github.com/facebookresearch/dinov3">DINOv3</a> by Meta AI.</p>
</footer>

</body>
</html>
